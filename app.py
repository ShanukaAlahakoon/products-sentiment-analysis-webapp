import streamlit as st
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import torch.nn.functional as F

# 1. ‡∂¥‡∑í‡∂ß‡∑î‡∑Ä‡∑ö ‡∑É‡∑ê‡∂ö‡∑É‡∑î‡∂∏‡∑ä (Page Config)
st.set_page_config(
    page_title="Sentiment Analyzer",
    page_icon="ü§ñ",
    layout="centered"
)

# 2. ‡∂∏‡∑ú‡∂©‡∂Ω‡∑ä ‡∂ë‡∂ö Load ‡∂ö‡∂ª‡∂ú‡∑ê‡∂±‡∑ì‡∂∏ (Cache ‡∂ö‡∂ª‡∂±‡∑Ä‡∑è ‡∑Ä‡∑ö‡∂ú‡∑Ä‡∂≠‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±)
@st.cache_resource
def load_model():
    # ‡∂∏‡∑ú‡∂©‡∂Ω‡∑ä ‡∂ë‡∂ö ‡∂≠‡∑í‡∂∫‡∑ô‡∂± ‡∂≠‡∑ê‡∂± (‡∂Ö‡∂¥‡∑í ‡∑Ñ‡∂Ø‡∂¥‡∑î folder ‡∂ë‡∂ö)
    model_path = "./model"
    
    # CPU ‡∂ë‡∂ö ‡∂¥‡∑è‡∑Ä‡∑í‡∂†‡∑ä‡∂†‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂± (Server ‡∂ë‡∂ö‡∑ö GPU ‡∂±‡∑ê‡∂≠‡∑í ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑ä ‡∂±‡∑í‡∑É‡∑è)
    device = torch.device('cpu')
    
    tokenizer = BertTokenizer.from_pretrained(model_path)
    model = BertForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    
    return tokenizer, model, device

# Loading message ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂¥‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑Ä‡∑è
with st.spinner('Loading AI Model... Please wait...'):
    tokenizer, model, device = load_model()

# 3. Web App ‡∂ë‡∂ö‡∑ö ‡∂¥‡∑ô‡∂±‡∑î‡∂∏ (UI Design)
st.title("ü§ñ AI Review Analyzer")
st.markdown("Type a review below to see if it's **Positive** or **Negative**!")

# User ‡∂ß type ‡∂ö‡∂ª‡∂±‡∑ä‡∂± ‡∂â‡∂© ‡∂Ø‡∑ô‡∂±‡∑Ä‡∑è
user_input = st.text_area("Enter your text here:", height=150, placeholder="Example: This product is amazing!")

# Button ‡∂ë‡∂ö click ‡∂ö‡∑Ö‡∑è‡∂∏ ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂ï‡∂± ‡∂Ø‡∑ö
if st.button("Analyze Sentiment"):
    if user_input.strip() == "":
        st.warning("Please enter some text first!")
    else:
        # 4. Prediction Logic (‡∂Ö‡∂¥‡∑í Colab ‡∂ë‡∂ö‡∑ö ‡∂Ω‡∑í‡∂∫‡∂¥‡∑î ‡∂ë‡∂ö‡∂∏‡∂∫‡∑í)
        encoded_review = tokenizer.encode_plus(
            user_input,
            max_length=128,
            add_special_tokens=True,
            return_token_type_ids=False,
            pad_to_max_length=True, # ‡∂Ö‡∂Ω‡∑î‡∂≠‡∑ä versions ‡∑Ä‡∂Ω padding='max_length' ‡∑Ñ‡∑ú‡∂≥‡∂∫‡∑í
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt',
            truncation=True
        )

        input_ids = encoded_review['input_ids'].to(device)
        attention_mask = encoded_review['attention_mask'].to(device)

        with torch.no_grad():
            output = model(input_ids, attention_mask=attention_mask)

        probs = F.softmax(output.logits, dim=1)
        _, prediction = torch.max(probs, dim=1)
        
        confidence = probs.max().item() * 100
        
        # ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂µ‡∂Ω‡∂∫ ‡∂¥‡∑ô‡∂±‡∑ä‡∑Ä‡∑ì‡∂∏
        st.markdown("---")
        if prediction.item() == 1:
            st.success(f"### Result: Positive üòÉ")
            st.write(f"Confidence: **{confidence:.2f}%**")
        else:
            st.error(f"### Result: Negative üòû")
            st.write(f"Confidence: **{confidence:.2f}%**")